# crawler/crawler.py
import time
import requests
from bs4 import BeautifulSoup
from crawler.robots import can_crawl
from crawler.db_connection import get_redis_client

# åˆå§‹åŒ– Redis å®¢æˆ·ç«¯
redis_client = get_redis_client()

# ä¼ªè£…æˆçœŸå®æµè§ˆå™¨çš„è¯·æ±‚å¤´
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36"
}

# å®šä¹‰çˆ¬å–æœ€å¤§æ·±åº¦ï¼Œé¿å…æ— é™é€’å½’
MAX_DEPTH = 3

def crawl(depth=0):
    if depth > MAX_DEPTH:
        print("â›” çˆ¬å–æ·±åº¦è¶…é™ï¼Œåœæ­¢é€’å½’")
        return

    # ä» Redis ä¸­å–å‡ºä¸€ä¸ªå¾…çˆ¬å– URL
    url_to_crawl = redis_client.spop("pending_urls")
    if not url_to_crawl:
        print("ğŸš€ æ²¡æœ‰æ–°çš„ URL éœ€è¦çˆ¬å–")
        return

    # åœ¨è¯·æ±‚å‰å…ˆæ£€æŸ¥ robots.txt æ˜¯å¦å…è®¸çˆ¬å–
    if not can_crawl(url_to_crawl, "MyCrawler"):
        print(f"âŒ {url_to_crawl} ä¸å…è®¸çˆ¬å– (robots åè®®ç¦æ­¢)")
        return

    print(f"ğŸ“Œ çˆ¬å–: {url_to_crawl} (æ·±åº¦: {depth})")

    try:
        response = requests.get(url_to_crawl, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            # æå–é¡µé¢ä¸­æ‰€æœ‰é“¾æ¥
            for link in soup.find_all("a", href=True):
                new_url = link["href"]
                # å¦‚æœå…è®¸çˆ¬å–ï¼Œå¹¶ä¸”æ–° URL æ˜¯æˆ‘ä»¬å…³æ³¨çš„é¡µé¢ï¼Œå°±åŠ å…¥ Redisï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                if can_crawl(new_url, "MyCrawler") and new_url.startswith("https://www.douban.com/group/topic/"):
                    redis_client.sadd("pending_urls", new_url)
                    print(f"â• å‘ç°æ–° URL: {new_url}")
            print(f"âœ… çˆ¬å–å®Œæˆ: {url_to_crawl}")
        else:
            print(f"âŒ è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ çˆ¬å–å‡ºé”™: {e}")

    # æ§åˆ¶çˆ¬å–é€Ÿåº¦ï¼Œé˜²æ­¢è¢«å° IP
    time.sleep(3)
    # é€’å½’è°ƒç”¨ï¼Œçˆ¬å–ä¸‹ä¸€å±‚
    crawl(depth + 1)
