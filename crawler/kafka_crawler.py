import os
import time
import json
import requests
import mysql.connector
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from urllib import robotparser

# Kafka-python åº“ï¼Œç”¨äºç”Ÿäº§ & æ¶ˆè´¹ Kafka æ¶ˆæ¯
from kafka import KafkaConsumer, KafkaProducer

# =========== DNSè§£æç›¸å…³(æ–°å¢) ===========
# å‡è®¾ dns_resolver.py ä¸æœ¬æ–‡ä»¶åœ¨åŒçº§ç›®å½•ä¸‹
from dns_resolver import extract_domain, resolve_hostname

# =============== 1. åŠ è½½ .env ===============
load_dotenv(dotenv_path="C:\\Users\\Einstein\\Documents\\GitHub\\scrapy-redis\\.env", override=True)  # ä¼šè¯»å–å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ï¼Œä½ ä¹Ÿå¯æŒ‡å®šç»å¯¹è·¯å¾„
MYSQL_HOST = os.getenv("MYSQL_HOST", "localhost")
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "crawler_db")

print("æ£€æŸ¥ MySQL:", MYSQL_HOST, MYSQL_USER)

# =============== 2. è¿æ¥ MySQL ===============
conn = mysql.connector.connect(
    host=MYSQL_HOST,
    user=MYSQL_USER,
    password=MYSQL_PASSWORD,
    database=MYSQL_DATABASE
)
cursor = conn.cursor()

# å»ºè¡¨ï¼ˆè‹¥æ— åˆ™åˆ›å»ºï¼‰
cursor.execute("""
CREATE TABLE IF NOT EXISTS crawled_data (
    id INT AUTO_INCREMENT PRIMARY KEY,
    url TEXT,
    title TEXT,
    content LONGTEXT,
    crawled_time DATETIME,
    UNIQUE KEY url_unique (url(255))
);
""")

def save_to_mysql(url, title, content):
    """
    å°†æŠ“å–åˆ°çš„ (url, title, content) å†™å…¥ MySQLã€‚
    ä½¿ç”¨INSERT IGNOREé¿å…é‡å¤æ’å…¥ï¼ˆéœ€urlä¸Šæœ‰UNIQUEç´¢å¼•ï¼‰ã€‚
    """
    sql = """
        INSERT IGNORE INTO crawled_data (url, title, content, crawled_time)
        VALUES (%s, %s, %s, %s)
    """
    cursor.execute(sql, (url, title, content, datetime.now()))
    conn.commit()

def already_crawled(url):
    """
    ç®€å•åœ°æŸ¥è¯¢ MySQL æ˜¯å¦å·²å­˜åœ¨è¯¥URLã€‚
    è‹¥æ‰¾åˆ°ï¼Œåˆ™è¿”å› Trueã€‚å¦åˆ™ Falseã€‚
    """
    sql = "SELECT id FROM crawled_data WHERE url=%s"
    cursor.execute(sql, (url,))
    row = cursor.fetchone()
    return row is not None

# =============== 3. æœºå™¨äººåè®®æ£€æŸ¥ ===============
def can_crawl(url, user_agent="MyCrawler"):
    # ä½ åŸå…ˆå†™æ­»: "https://www.douban.com/robots.txt"
    # å¦‚æœä½ çˆ¬å¤šç«™ç‚¹ï¼Œéœ€è¦å…ˆè§£æurlè·å–domainå†æ‹¼robots.txt
    robots_url = "https://www.douban.com/robots.txt"
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)
    rp.read()
    return rp.can_fetch(user_agent, url)

# =============== 4. ä¼ªè£… & Cookies ===============
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

cookies = {
    "dbcl2": "287079700:Fc39rI5JVZs",
    "ck": "FIRV"
}

# =============== 5. Kafka Producer ===============
producer = KafkaProducer(
    bootstrap_servers=["localhost:9092"],  # æ›¿æ¢ä¸ºä½ çš„ Kafka broker åœ°å€
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)

def send_url_task(url, depth=0):
    """
    å¾€Kafkaçš„ 'url_task' ä¸»é¢˜å‘é€æ–°çš„urlä»»åŠ¡: {url, depth}
    """
    msg = {"url": url, "depth": depth}
    producer.send("url_task", msg)
    producer.flush()
    print(f"ã€Producerã€‘å·²å‘é€ URL={url}, depth={depth}")

# =============== 6. çˆ¬è™«é€»è¾‘ ===============
MAX_DEPTH = 3

def process_url_task(task):
    """
    å¤„ç†ä¸€æ¡æ¥è‡ªKafkaçš„æ¶ˆæ¯, æ ¼å¼ { "url":..., "depth":...}
    1. æ£€æŸ¥æ·±åº¦
    2. æ£€æŸ¥ robots
    3. æ£€æŸ¥æ˜¯å¦å·²çˆ¬
    4. å‘è¯·æ±‚å¹¶è§£æ -> å†™MySQL
    5. å‘ç°æ–°é“¾æ¥ -> å‘é€å›Kafka
    """
    url = task.get("url")
    depth = task.get("depth", 0)

    if not url:
        return
    if depth > MAX_DEPTH:
        print(f"â›” è¶…è¿‡æœ€å¤§æ·±åº¦, è·³è¿‡: {url}")
        return

    # å…ˆæ£€æŸ¥ robots
    if not can_crawl(url):
        print(f"âŒ æœºå™¨äººåè®®ç¦æ­¢: {url}")
        return

    # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬è¿‡
    if already_crawled(url):
        print(f"âš ï¸ å·²çˆ¬è¿‡, è·³è¿‡: {url}")
        return
    
# =========== DNSè§£æå¼€å§‹ (æ ¸å¿ƒæ–°å¢) ===========
    domain = extract_domain(url)
    try:
        ip_list = resolve_hostname(domain)
        print(f"ğŸŒ DNSè§£ææˆåŠŸ: {domain} -> {ip_list}")
    except Exception as e:
        print(f"âŒ DNSè§£æå¤±è´¥: {domain}, é”™è¯¯: {e}")
        # å¯ä»¥é€‰æ‹©returnï¼Œä¹Ÿå¯ä»¥ç»§ç»­å°è¯•å‘é€è¯·æ±‚ï¼Œçœ‹ä½ éœ€æ±‚ã€‚
        # return
    # =========== DNSè§£æç»“æŸ ===========


    print(f"ğŸ“Œ æ­£åœ¨çˆ¬å–: {url} (depth={depth})")
    try:
        resp = requests.get(url, headers=headers, cookies=cookies, timeout=10)
        if resp.status_code == 200:
            # æå–æ ‡é¢˜å’Œæ­£æ–‡
            soup = BeautifulSoup(resp.text, "html.parser")
            title = soup.title.get_text(strip=True) if soup.title else "æ— æ ‡é¢˜"
            body = soup.find("body")
            content_text = body.get_text("\n", strip=True) if body else "æ— å†…å®¹"

            # å†™è¿›MySQL
            save_to_mysql(url, title, content_text)

            # æå–æ‰€æœ‰å­é“¾æ¥
            for link in soup.find_all("a", href=True):
                new_url = link["href"].strip()
                # æ ¹æ®ä½ çš„éœ€æ±‚, åªçˆ¬æŒ‡å®šèŒƒå›´. è¿™é‡Œç¤ºä¾‹è±†ç“£ group topic
                if new_url.startswith("https://www.douban.com/group/topic/"):
                    # æŠŠå®ƒå‘é€å›Kafka, depth+1
                    send_url_task(new_url, depth+1)

            print(f"âœ… çˆ¬å–å®Œæˆ: {url}")
        else:
            print(f"âŒ çŠ¶æ€ç ={resp.status_code}, url={url}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

# =============== 7. Kafka Consumer å¾ªç¯æ¶ˆè´¹ ===============
consumer = KafkaConsumer(
    "url_task",
    bootstrap_servers=["localhost:9092"],  # æ›¿æ¢ä¸ºä½ çš„Kafka brokeråœ°å€
    group_id="my_crawler_group",           # å¤šèŠ‚ç‚¹æ—¶ç›¸åŒgroupå¯å¹¶è¡Œåˆ†é…
    auto_offset_reset="earliest",          # æ— offsetæ—¶, ä»å¤´å¼€å§‹æ¶ˆè´¹
    enable_auto_commit=True,
    value_deserializer=lambda m: json.loads(m.decode("utf-8"))
)

# =============== 8. ç¨‹åºå…¥å£ ===============
if __name__ == "__main__":
    # 1) å‘é€ä¸€äº›ç§å­URL (å¯é€‰: åªéœ€ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶åŠ ç§å­)
    #    ä½ å¯æ³¨é‡Šæ‰æˆ–åªåœ¨æŸå°æœºå™¨æ‰§è¡Œä¸€æ¬¡
    seed_urls = [
        "https://www.douban.com/group/586674/"
    ]
    for su in seed_urls:
        send_url_task(su, 0)

    # 2) ä¸æ–­ä»Kafkaæ¶ˆè´¹æ¶ˆæ¯ -> çˆ¬å–
    print("=== å¼€å§‹æ¶ˆè´¹ 'url_task' ä¸»é¢˜ ===")
    try:
        for msg in consumer:
            task_data = msg.value  # {url, depth}
            process_url_task(task_data)
    except KeyboardInterrupt:
        print("=== åœæ­¢æ¶ˆè´¹ ===")
    finally:
        consumer.close()
        producer.close()
        cursor.close()
        conn.close()

